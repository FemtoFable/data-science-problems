{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMT309 - Computational Data Science - Data Science Portfolio**"
      ],
      "metadata": {
        "id": "t_hYgOgUzIwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Part 1 - Text Data Analysis (45 marks)\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
        "\n",
        "In this exercise, you are asked to perform a number of operations to gain insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.0) Suggested/Required Imports"
      ],
      "metadata": {
        "id": "jlssd-CQOOXE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967bb260-980b-412d-8832-09054ac69ed1"
      },
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b2b886-30e8-401e-b22c-07cce7e7642f"
      },
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "df = pd.read_csv('data_portfolio_22.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_22.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pym3RVnR1M4W",
        "outputId": "61a5a70d-9e4e-478d-c3f1-7443a0927dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19940, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "CNfbxg2X3nzK",
        "outputId": "ba40e419-94f0-4f56-a336-be0e7b8f454f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-763dab04-d8f8-4888-a02b-8479aeddc50f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-763dab04-d8f8-4888-a02b-8479aeddc50f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-763dab04-d8f8-4888-a02b-8479aeddc50f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-763dab04-d8f8-4888-a02b-8479aeddc50f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing (20 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpA4Z5ADdok"
      },
      "source": [
        "### P1.1.1 - Offensive authors per subreddit (5 marks)\n",
        "\n",
        "As you will see, the dataset contains a lot of strings of the form `[***]`. These have been used to mask (or remove) swearwords to make it less offensive. We are interested in finding those users that have posted at least one swearword in each subreddit. We do this by counting occurrences of the `[***]` string in the `selftext` column (we can assume that an occurrence of `[***]` equals a swearword in the original dataset).\n",
        "\n",
        "**What to implement:** A function `offensive_authors(df)` that takes as input the original dataframe and returns a dataframe of the form below, where each row contains authors that posted at least one swearword in the corresponding subreddit.\n",
        "\n",
        "```\n",
        "subreddit\tauthor\n",
        "0\t40kLore\tCross_Ange\n",
        "1\t40kLore\tDaRandomGitty2\n",
        "2\t40kLore\tEMB1981\n",
        "3\t40kLore\tEvoxrus_XV\n",
        "4\t40kLore\tGrtrshop\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def offensive_authors(df):\n",
        "  \"\"\"\n",
        "  Identify authors with offensive posts across different subreddits.\n",
        "    \n",
        "  This function takes a DataFrame containing posts data (with 'author', 'subreddit', and 'selftext' columns) \n",
        "  and returns a DataFrame with the authors who have posted at least one offensive post in each subreddit.\n",
        "    \n",
        "  Input:\n",
        "  df (pd.DataFrame): A DataFrame containing posts data with 'author', 'subreddit', and 'selftext' columns.\n",
        "\n",
        "  Output:\n",
        "  result (pd.DataFrame): A DataFrame with columns 'subreddits' and 'author', listing the authors who have posted\n",
        "                            at least one offensive post in each subreddit, sorted by subreddit.\n",
        "  \"\"\"\n",
        "\n",
        "  df1 = df.copy() \n",
        "  # Replace '[***]' with a unique placeholder string that won't occur in any selftext, as [***] can be inside words\n",
        "  df1['selftext'] = df1['selftext'].str.replace(r'\\[.*?\\*\\*\\*.*?\\]', '###OFFENSIVE###')\n",
        "\n",
        "\n",
        "  # Create a dictionary of {author: set of subreddits with offensive posts}\n",
        "  author_subreddits = {}\n",
        "  for author, subreddit, selftext in zip(df1['author'], df1['subreddit'], df1['selftext']):\n",
        "      if '###OFFENSIVE###' in selftext:\n",
        "          if author not in author_subreddits:\n",
        "              author_subreddits[author] = set()\n",
        "          author_subreddits[author].add(subreddit)\n",
        "    \n",
        "\n",
        "    \n",
        "  # Find authors with at least one offensive post in each subreddit\n",
        "  result = []\n",
        "  for author, subreddits in author_subreddits.items():\n",
        "      if len(subreddits) > 0:\n",
        "          result.append((subreddits, author))\n",
        "         \n",
        "\n",
        "  result = pd.DataFrame(result, columns=['subreddits', 'author'])\n",
        "  result = result.explode('subreddits')                                 #expand multiple subreddits per author into individual rows\n",
        "  result = result.sort_values(by='subreddits').reset_index(drop = True)   \n",
        "                     \n",
        "    \n",
        "  return result"
      ],
      "metadata": {
        "id": "9oby6Xbz59F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offensive_authors(df)"
      ],
      "metadata": {
        "id": "FdiKSgWa9dKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "46d6febc-29cc-4b39-9d13-885ce9659352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        subreddits              author\n",
              "0          40kLore             EMB1981\n",
              "1          40kLore          SlobBarker\n",
              "2          40kLore        spirtomb1831\n",
              "3          40kLore          ThePoarter\n",
              "4          40kLore        ThereGoesJoe\n",
              "..             ...                 ...\n",
              "490  worldbuilding     Drake[***]zilla\n",
              "491  worldbuilding        HorsesPlease\n",
              "492  worldbuilding          BeardedJho\n",
              "493          xqcow            marvi444\n",
              "494          xqcow  Stubka_The_Russian\n",
              "\n",
              "[495 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7d3ddfb-b842-45ff-be1c-f767b11aab78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddits</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>EMB1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>SlobBarker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>spirtomb1831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>ThePoarter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40kLore</td>\n",
              "      <td>ThereGoesJoe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>Drake[***]zilla</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>HorsesPlease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>worldbuilding</td>\n",
              "      <td>BeardedJho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>xqcow</td>\n",
              "      <td>marvi444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>xqcow</td>\n",
              "      <td>Stubka_The_Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>495 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7d3ddfb-b842-45ff-be1c-f767b11aab78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7d3ddfb-b842-45ff-be1c-f767b11aab78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7d3ddfb-b842-45ff-be1c-f767b11aab78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Most common trigrams per subreddit (15 marks)\n",
        "\n",
        "We are interested in learning about _the ten most frequent trigrams_ (a [trigram](https://en.wikipedia.org/wiki/Trigram) is a sequence of three consecutive words) in each subreddit's content. You must compute these trigrams on both the `selftext` and `title` columns. Your task is to generate a Python dictionary of the form:\n",
        "\n",
        "```\n",
        "{subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "subreddit1: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],\n",
        "...\n",
        "subreddit63: [(trigram1, freq1), (trigram2, freq2), ... , (trigram3, freq10)],}\n",
        "```\n",
        "\n",
        "That is, for each subreddit, the 10 most frequent trigrams and their frequency, stored in a list of tuples. Each trigram will be stored also as a tuple containing 3 strings.\n",
        "\n",
        "**What to implement**: A function `get_tris(df, stopwords_list, punctuation_list)` that will take as input the original dataframe, a list of stopwords and a list of punctuation signs (e.g., `?` or `!`), and will return a python dictionary with the above format. Your function must implement the following steps in order:\n",
        "\n",
        "- (**1 mark**) Create a new dataframe called `newdf` with only `subreddit`, `title` and `selftext` columns.\n",
        "- (**1 mark**) Add a new column to `newdf` called `full_text`, which will contain `title` and `selftext` concatenated with the string `.` (a full stop) followed by a space. That, is `A simple title` and `This is a text body` would be `A simple title. This is a text body`.\n",
        "- (**1 mark**) Remove all occurrences of the following strings from `full_text`. You must do this without creating a new column:\n",
        "  - `[***]`\n",
        "  - `&amp;`\n",
        "  - `&gt;`\n",
        "  - `https`\n",
        "- (**1 mark**) You must also remove all occurrences of at least three consecutive hyphens, for example, you should remove strings like `---`, `----`, `-----`, etc., but not `--` and not `-`.\n",
        "- (**1 mark**) Tokenize the contents of the `full_text` column after lower casing (removing all capitalization). You should use the `word_tokenize` function in `nltk`. Add the results to a new column called `full_text_tokenized`.\n",
        "- (**2 mark**) Remove all tokens that are either stopwords or punctuation from `full_text_tokenized` and store the results in a new column called `full_text_tokenized_clean`. _See Note 1_.\n",
        "- (**2 marks**) Create a new dataframe called `adf` (which will stand for _aggregated dataframe_), which will have one row per subreddit (i.e., 63 rows), and will have two columns: `subreddit` (the subreddit name), and `all_words`, which will be a big list with all the words that belong to that subreddit as extracted from the `full_text_tokenized_clean`.\n",
        "- (**3 marks**) Obtain trigram counts, which will be stored in a dictionary where each `key` will be a trigram (a `tuple` containing 3 consecutive tokens), and each `value` will be their overall frequency in that subreddit. You are  encouraged to use functions from the `nltk` package, although you can choose any approach to solve this part.\n",
        "- (**3 marks**) Finally, use the information you have in `adf` for generating the desired dictionary, and return it. _See Note 2_.\n",
        "\n",
        "Note 1. You can obtain stopwords and punctuation as follows.\n",
        "- Stopwords: \n",
        "```\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "```\n",
        "- Punctuation:\n",
        "```\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "```\n",
        "\n",
        "Note 2. You do not have to apply an additional ordering when there are several trigrams with the same frequency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports here for extra clarity\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "def get_tris(df, stopwords_list, punctuation_list):\n",
        "  \"\"\"\n",
        "  Extract the top 10 most frequent trigrams from each subreddit in a DataFrame.\n",
        "    \n",
        "  This function takes a DataFrame containing posts data (with 'subreddit', 'title', and 'selftext' columns),\n",
        "  a list of stopwords, and a list of punctuation marks. It processes the text and returns a dictionary with the\n",
        "  top 10 most frequent trigrams in each subreddit.\n",
        "    \n",
        "  Input:\n",
        "  df (pd.DataFrame): A DataFrame containing posts data with 'subreddit', 'title', and 'selftext' columns.\n",
        "  stopwords_list (list): A list of stopwords to exclude from the text processing.\n",
        "  punctuation_list (list): A list of punctuation marks to exclude from the text processing.\n",
        "    \n",
        "  Output:\n",
        "  res (dict): A dictionary where keys are subreddit names and values are lists of the top 10 most frequent trigrams\n",
        "                in each subreddit, sorted in descending order of frequency.\n",
        "  \"\"\"\n",
        "\n",
        "  # 1 MARK - create new df with only relevant columns\n",
        "  newdf = df.loc[:,['subreddit','title','selftext']]\n",
        "  \n",
        "  # 1 MARK - concatenate title and selftext\n",
        "  newdf['full_text'] = newdf['title']+'. '+newdf['selftext']\n",
        "  \n",
        "  # 1 MARK for string replacement: remove the strings \"[***]\", \"&amp;\", \"&gt;\" and \"https\"\n",
        "  newdf['full_text'] = newdf['full_text'].replace(r'\\[.*?\\]', '', regex=True)\n",
        "  newdf['full_text'] = newdf['full_text'].replace('&amp;', '', regex=True)\n",
        "  newdf['full_text'] = newdf['full_text'].replace('&gt;', '', regex=True)\n",
        "  newdf['full_text'] = newdf['full_text'].replace('https', '', regex=True)\n",
        "  \n",
        "\n",
        "  # 1 MARK for regex replacement: at least three consecutive dashes\n",
        "  newdf['full_text'] = newdf['full_text'].replace(r'[-]{3,}', '', regex=True)\n",
        "  \n",
        "  # 1 MARK - lower case, tokenize, and add result to full_text_tokenize\n",
        "  newdf['full_text_tokenized'] = newdf['full_text'].str.lower()\n",
        "  newdf['full_text_tokenized'] = newdf['full_text_tokenized'].apply(nltk.word_tokenize)\n",
        "  \n",
        "  # 2 MARKS - clean the full_text_tokenized column by iterating over each word and discarding if it's either a stopword or punctuation\n",
        "\n",
        "  sw = stopwords_list\n",
        "  p = punctuation_list\n",
        "\n",
        "  # apply function to full_text_tokenized column\n",
        "  newdf['full_text_tokenized_clean'] = newdf['full_text_tokenized'].apply(lambda x: [token for token in x if token not in sw and token not in p])\n",
        "\n",
        "  # 2 MARKS - create new aggregated dataframe by concatenating all full_text_tokenized_clean values - rename columns as requested\n",
        "\n",
        "  adf = newdf.groupby('subreddit')['full_text_tokenized_clean'].apply(lambda x: [item for sublist in x for item in sublist]).reset_index()\n",
        "  adf.columns = ['subreddit', 'all_words']\n",
        "  \n",
        "  # 3 MARKS - create new Series object by piping nltk's FreqDist and trigrams functions into all_words\n",
        "\n",
        "  tri_counts = adf['all_words'].apply(lambda x: nltk.FreqDist(nltk.trigrams(x)))\n",
        "  \n",
        "  # 3 MARKS - create output dictionary by zipping subreddit column from adf and tri_counts into a list of tuples, then passing dict()\n",
        "  \n",
        "  # the top 10 most frequent ngrams are obtained by calling sorted() on tri_counts and keeping only the top 10 elements\n",
        "\n",
        "  res = dict(zip(adf['subreddit'], [sorted(counts.items(), key=lambda x: x[1], reverse=True)[:10] for counts in tri_counts]))\n",
        "  return res\n",
        "  "
      ],
      "metadata": {
        "id": "A0NeN7uGftfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords as list\n",
        "sw = sw.words('english')\n",
        "# get punctuation as list\n",
        "p = list(string.punctuation)\n",
        "# optional lines for adding the below line to avoid the SettingWithCopyWarning\n",
        "warnings.filterwarnings('ignore')\n",
        "get_tris(df, sw, p)"
      ],
      "metadata": {
        "id": "93vyGGMjhrSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas (15 marks)\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Authors that post highly commented posts (3 marks)\n",
        "\n",
        "Find the top 1000 most commented posts. Then, obtain the names of the authors that have at least 3 posts among these posts.\n",
        "\n",
        "**What to implement:** Implement a function `find_popular_authors(df)` that takes as input the original dataframe and returns a list strings, where each string is the name of authors that satisfy the above criteria."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_popular_authors(df):\n",
        "  \"\"\"\n",
        "  Identify authors with at least 3 posts in the top 1000 most commented posts in a DataFrame.\n",
        "\n",
        "  This function takes a DataFrame containing post data (with 'author' and 'num_comments' columns) and\n",
        "  finds authors who have at least 3 posts among the top 1000 most commented posts. The resulting\n",
        "  list of author names is sorted alphabetically in ascending order.\n",
        "\n",
        "  Input:\n",
        "  df (pd.DataFrame): A DataFrame containing post data with 'author' and 'num_comments' columns.\n",
        "\n",
        "  Output:\n",
        "  names (list): A sorted list of author names with at least 3 posts in the top 1000 most commented posts.\n",
        "  \"\"\"\n",
        "\n",
        "  top = df.nlargest(1000, 'num_comments') #finds top 1000 most commented posts\n",
        "  \n",
        "  \n",
        "  count = {}                              #this dictionary will contain author names and their number of comments left\n",
        "  for name, value in top['author'].iteritems(): \n",
        "    if value in count:\n",
        "      count[value] += 1\n",
        "    else:\n",
        "      count[value] = 1\n",
        "\n",
        "  names = []                              #filters out authors with at least 3 comments\n",
        "  for author in count:\n",
        "    if count[author] > 2:\n",
        "      names.append(author)\n",
        "    else:\n",
        "      continue\n",
        "  \n",
        "  names.sort(key=str.lower)\n",
        "  return names\n"
      ],
      "metadata": {
        "id": "URKIW6oMvrYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_popular_authors(df)"
      ],
      "metadata": {
        "id": "tmsJyZ1_xpGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c774d2c8-f35f-4c8c-cf79-ba2fb8ed2bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020c[***]er[***]',\n",
              " '[***]reader',\n",
              " 'akarim5847',\n",
              " 'allicat83',\n",
              " 'AllisonGator',\n",
              " 'Allstarhit',\n",
              " 'Antiliani',\n",
              " 'apocalypticalley',\n",
              " 'AristonD',\n",
              " 'AutoModerator',\n",
              " 'BanDerUh',\n",
              " 'BebeFanMasterJ',\n",
              " 'bemani4u',\n",
              " 'bgny',\n",
              " 'blacked_lover',\n",
              " 'boomerpro',\n",
              " 'CantStopPoppin',\n",
              " 'cdillon42',\n",
              " 'chakalakasp',\n",
              " 'CLO_Junkie',\n",
              " 'Cross_Ange',\n",
              " 'DaFunkJunkie',\n",
              " 'Defie-LOH-Gic',\n",
              " 'dsbwayne',\n",
              " 'dukey',\n",
              " 'dunphish64',\n",
              " 'elt0p0',\n",
              " 'epiphanyx99',\n",
              " 'faab64',\n",
              " 'foodforthinks',\n",
              " 'Fr1sk3r',\n",
              " 'Frocharocha',\n",
              " 'FunPeach0',\n",
              " 'Gambit08',\n",
              " 'habichuelacondulce',\n",
              " 'harushiga',\n",
              " 'hildebrand_rarity',\n",
              " 'hilltopye',\n",
              " 'HippolasCage',\n",
              " 'imagepoem',\n",
              " 'into_the_[***]e',\n",
              " 'invertedparado[***]',\n",
              " 'iSlingShlong',\n",
              " 'itsreallyreallytrue',\n",
              " 'Jellyrollrider',\n",
              " 'jigsawmap',\n",
              " 'johnruby',\n",
              " 'jollygreenscott91',\n",
              " 'KatieAllTheTime',\n",
              " 'kevinmrr',\n",
              " 'Kinmuan',\n",
              " 'kogeliz',\n",
              " 'lanqian',\n",
              " 'le_br1t',\n",
              " 'Leg_holes',\n",
              " 'lilmcfuggin',\n",
              " 'Lost_Distribution546',\n",
              " 'Lshim',\n",
              " 'Madd-Nigrulo',\n",
              " 'madman320',\n",
              " 'Mahomeboy_',\n",
              " 'Majnum',\n",
              " 'MakeItRainSheckels',\n",
              " 'MisterT12',\n",
              " 'Morihando',\n",
              " 'mouthofreason',\n",
              " 'MrRoxx',\n",
              " 'mythrowawaybabies',\n",
              " 'None',\n",
              " 'Not4Reel',\n",
              " 'notpreposterous',\n",
              " 'nycsellit4me',\n",
              " 'NYLaw',\n",
              " 'OgranismAtWork',\n",
              " 'OldFashionedJizz',\n",
              " 'oliver_21',\n",
              " 'Playaguy',\n",
              " 'PlenitudeOpulence',\n",
              " 'puppuli',\n",
              " 'r[***]og',\n",
              " 'Ramy_',\n",
              " 'ratioetlogicae',\n",
              " 'rebooted_life_42',\n",
              " 'ReginaldJohnston',\n",
              " 'Romano16',\n",
              " 'Saibasaurus',\n",
              " 'Salramm01',\n",
              " 'samzz41',\n",
              " 'schuey_08',\n",
              " 'SemperPereunt',\n",
              " 'Singularitytracker',\n",
              " 'SlobBarker',\n",
              " 'somnifacientsawyer',\n",
              " 'stargem5',\n",
              " 'stealthyfrog',\n",
              " 'Stoaticor',\n",
              " 'strngerdngermaus',\n",
              " 'SUPERGUESSOUS',\n",
              " 'tacolben',\n",
              " 'TAKEitTOrCIRCLEJERK',\n",
              " 'tefunka',\n",
              " 'TheFearlessWarrior',\n",
              " 'theitguyforever',\n",
              " 'TurtleFacts72',\n",
              " 'twistedlogicx',\n",
              " 'Typoqueen00',\n",
              " 'ufgman',\n",
              " 'UpNDownCan',\n",
              " 'vizard673',\n",
              " 'Wagamaga',\n",
              " 'werdmouf',\n",
              " 'WorkTomorrow',\n",
              " 'XDitto',\n",
              " 'Zhana-Aul']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrl0kq09dxrp"
      },
      "source": [
        "### P1.2.2 - Distribution of posts per weekday (5 marks)\n",
        "\n",
        "Find the percentage of posts that were posted in each weekday (Monday, Tuesday, etc.). You can use an external calendar or you can use any functionality for dealing with dates available in pandas. \n",
        "\n",
        "**What to implement:** A function `get_weekday_post_distribution(df)` that takes as input the original dataframe and returns a dictionary of the form (the values are made up):\n",
        "\n",
        "```\n",
        "{'Monday': '14%',\n",
        "'Tuesday': '23%', \n",
        "...\n",
        "}\n",
        "```\n",
        "\n",
        "Note that you must only return two decimals, and you must include the percentage sign in the output dictionary. \n",
        "\n",
        "Note that in dictionaries order is not preserved, so the order in which it gets printed will not matter. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weekday_post_distribution(df):\n",
        "  \"\"\"\n",
        "  Function to compute the distribution of posts across weekdays in a given DataFrame.\n",
        "\n",
        "  Input:\n",
        "  df (pd.DataFrame): A DataFrame containing a 'posted_at' column with timestamps.\n",
        "\n",
        "  Output:\n",
        "  res (dict): A dictionary with keys as the day of the week and values as the percentage of posts on that day.\n",
        "  \"\"\"\n",
        "\n",
        "  df['date'] = pd.to_datetime(df['posted_at'])\n",
        "\n",
        "  df['day_of_week'] = df['date'].dt.day_name()\n",
        "\n",
        "  percentage_per_day = (df['day_of_week'].value_counts(normalize=True) * 100).round(2).astype(str) + '%' #number of decimals in output can be changed here \n",
        "\n",
        "  res = percentage_per_day.to_dict()\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "Aj_2ss9jy9WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weekday_post_distribution(df)"
      ],
      "metadata": {
        "id": "5FUpOzgN1t3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3069a9a-2210-4bf7-a791-261240324e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Wednesday': '14.89%',\n",
              " 'Friday': '14.79%',\n",
              " 'Thursday': '14.75%',\n",
              " 'Tuesday': '14.54%',\n",
              " 'Monday': '14.31%',\n",
              " 'Saturday': '13.76%',\n",
              " 'Sunday': '12.96%'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 - The 100 most passionate redditors (7 marks)\n",
        "\n",
        "We would like to know which are the 100 redditors (`author` column) that are most passionate. We will measure this by checking, for each redditor, the ratio at which they use adjectives. This ratio will be computed by dividing number of adjectives by the total number of words each redditor used. The analysis will only consider redditors that have written at least 1000 words.\n",
        "\n",
        "**What to implement:** A function called `get_passionate_redditors(df)` that takes as input the original dataframe and returns a list of the top 100 redditors (authors) by the ratio at which they use adjectives considering both the `title` and `selftext` columns. The returned list should be a list of tuples, where each inner tuple has two elements: the redditor (author) name, and the ratio of adjectives they used. The returned list should be sorted by adjective ratio in descending order (highest first). Only redditors that wrote more than 1000 words should be considered. You should use `nltk`'s `word_tokenize` and `pos_tag` functions to tokenize and find adjectives. You do not need to do any preprocessing like stopword removal, lemmatization or stemming."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_passionate_redditors(df):\n",
        "  \"\"\"\n",
        "  Function to find the top 100 passionate Redditors based on their adjective usage ratio.\n",
        "\n",
        "  Checks the ratio of adjectives used by each Redditor who has written at least 1000 words and outputs top 100 of them in \n",
        "  descending order along with their corresponding ratio.\n",
        "\n",
        "  Input:\n",
        "  df (pd.DataFrame): A DataFrame containing 'title', 'selftext', and 'author' columns.\n",
        "\n",
        "  Output:\n",
        "  top_100_list (list): A list of tuples containing the author's name and their adjective usage ratio.\n",
        "  \"\"\"\n",
        "\n",
        "  df['text'] = df['title'] + ' ' + df['selftext']\n",
        "  df['tokens'] = df['text'].apply(lambda x: word_tokenize(x)) #splits the sentences into words entry by entry\n",
        "    \n",
        "  # Compute word count and adjective count for each post\n",
        "  df['word_count'] = df['tokens'].apply(lambda x: len(x))\n",
        "  df['adj_count'] = df['tokens'].apply(lambda x: sum(1 for word, pos in pos_tag(x) if pos.startswith('JJ'))) ##JJ denotes adjectives\n",
        "    \n",
        "  # Group posts by author and compute total word count and adjective count for each author\n",
        "  grouped = df.groupby('author').agg({'word_count': 'sum', 'adj_count': 'sum'})    #need this to account for authors who wrote more than 1000 words across different posts\n",
        "    \n",
        "  # Filter out authors with less than 1000 words\n",
        "  grouped = grouped[grouped['word_count'] >= 1000]\n",
        "    \n",
        "  # Compute adjective ratio for each author\n",
        "  grouped['adj_ratio'] = grouped['adj_count'] / grouped['word_count']     \n",
        "    \n",
        "  # Sort authors by adjective ratio in descending order and displays top 100\n",
        "  sorted_grouped = grouped.sort_values(by='adj_ratio', ascending=False).round(3) \n",
        "  top_100 = sorted_grouped.head(100)\n",
        "    \n",
        "  return [(author, ratio) for author, ratio in top_100['adj_ratio'].items()]"
      ],
      "metadata": {
        "id": "Aizl96csDGeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_passionate_redditors(df)"
      ],
      "metadata": {
        "id": "7ddl-35Trg2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5461a6-19bc-41c9-9ea0-bd6f99f2aaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('OhanianIsTheBest', 0.148),\n",
              " ('healrstreettalk', 0.131),\n",
              " ('FreedomBoners', 0.125),\n",
              " ('factfind', 0.113),\n",
              " ('Travis-Cole', 0.106),\n",
              " ('fullbloodedwhitemale', 0.105),\n",
              " ('SecretAgentIceBat', 0.103),\n",
              " ('Tripmooney', 0.097),\n",
              " ('backpackwayne', 0.096),\n",
              " ('GeAlltidUpp', 0.096),\n",
              " ('mission_improbables', 0.095),\n",
              " ('EMB1981', 0.095),\n",
              " ('nyello-2000', 0.094),\n",
              " ('greyuniwave', 0.092),\n",
              " ('Venus230', 0.09),\n",
              " ('theinfinitelight', 0.088),\n",
              " ('th3allyK4t', 0.088),\n",
              " ('kent_k', 0.088),\n",
              " ('35quai', 0.088),\n",
              " ('120inn[***]', 0.088),\n",
              " ('notinferno', 0.087),\n",
              " ('Ninten-Doh', 0.086),\n",
              " ('kay278', 0.085),\n",
              " ('rrixham', 0.085),\n",
              " ('secretymology', 0.085),\n",
              " ('society0', 0.085),\n",
              " ('Zendexor', 0.084),\n",
              " ('spirtomb1831', 0.084),\n",
              " ('III_lll', 0.084),\n",
              " ('___TheKid___', 0.083),\n",
              " ('AnakinWayneII', 0.083),\n",
              " ('allofusahab', 0.083),\n",
              " ('sbpotdbot', 0.083),\n",
              " ('pooheygirl', 0.082),\n",
              " ('BornOnADifCloud', 0.082),\n",
              " ('anon7935678', 0.082),\n",
              " ('snorken123', 0.082),\n",
              " ('The_In-Betweener', 0.081),\n",
              " ('XDitto', 0.081),\n",
              " ('CommonEmployment2', 0.08),\n",
              " ('jmou3dxf', 0.08),\n",
              " ('cialu', 0.08),\n",
              " ('clemaneuverers', 0.08),\n",
              " ('NewTsahi', 0.079),\n",
              " ('FringeCenterPodcast', 0.079),\n",
              " ('kit8642', 0.078),\n",
              " ('Asshole411', 0.078),\n",
              " ('yellowsnow2', 0.078),\n",
              " ('bionista', 0.077),\n",
              " ('Stoaticor', 0.077),\n",
              " ('arthurmilchior', 0.077),\n",
              " ('dontbuyanylogos', 0.077),\n",
              " ('venCiere', 0.076),\n",
              " ('BlindingTwilight', 0.076),\n",
              " ('reddit_loves_pedos', 0.076),\n",
              " ('samueldon2020', 0.076),\n",
              " ('AutoModerator', 0.075),\n",
              " ('Long_on_AMD', 0.074),\n",
              " ('Kinmuan', 0.074),\n",
              " ('Cross_Ange', 0.074),\n",
              " ('Pretty_iin_Pink', 0.074),\n",
              " ('CuteBananaMuffin', 0.073),\n",
              " ('xrangegod1', 0.073),\n",
              " ('JeopardyGreen', 0.073),\n",
              " ('SlobBarker', 0.072),\n",
              " ('north0east', 0.072),\n",
              " ('Benster_ninja', 0.072),\n",
              " ('scamaltert', 0.071),\n",
              " ('Johnny21X', 0.071),\n",
              " ('truthesda', 0.071),\n",
              " ('blink3892938', 0.071),\n",
              " ('Grtrshop', 0.069),\n",
              " ('Rairaijin', 0.069),\n",
              " ('0naptoon', 0.069),\n",
              " ('Facts-Over-Opinion', 0.069),\n",
              " ('BK-Vatras', 0.069),\n",
              " ('austria9000', 0.069),\n",
              " ('bgny', 0.068),\n",
              " ('Collective1985', 0.068),\n",
              " ('bluesun100', 0.067),\n",
              " ('10100011a10100011a', 0.067),\n",
              " ('OmniusQubus', 0.066),\n",
              " ('DeadEndFred', 0.066),\n",
              " ('azagitova', 0.066),\n",
              " ('HorsesPlease', 0.065),\n",
              " ('coronaobserver', 0.065),\n",
              " ('Mrexreturns', 0.065),\n",
              " ('Cloud9Shopper', 0.064),\n",
              " ('FloundersEdition', 0.064),\n",
              " ('tari101190', 0.064),\n",
              " ('jdd7690', 0.064),\n",
              " ('balbs10', 0.063),\n",
              " ('ijoinedfor[***]Cx', 0.063),\n",
              " ('superiorpanda', 0.063),\n",
              " ('koolman631', 0.063),\n",
              " ('Daeani', 0.062),\n",
              " ('brevz123', 0.062),\n",
              " ('Drake[***]zilla', 0.062),\n",
              " ('LBC_Black_Cross', 0.062),\n",
              " ('DanishViking81', 0.061)]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1.3 Ethics (10 marks)"
      ],
      "metadata": {
        "id": "jQKJ4zc9UyOc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "Imagine you are **the head of a data mining company** that needs to use the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). Some\n",
        "information about the project and the team:\n",
        "\n",
        "- Your client is a political party concerned about misinformation.\n",
        "- The project requires mining Facebook, Reddit and Instagram data.\n",
        "- The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework.\n",
        "\n",
        "Your answer should address the following:\n",
        "- Identify the action in which your project is the weakest.\n",
        "- Then, justify your choice by critically analyzing the three key principles for that action outlined\n",
        "in the Framework, namely transparency, accountability and fairness.\n",
        "- Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "# **Answer:**\n",
        "\n",
        "The quality and limitations of the data are considered the weakest aspect of this project. To elucidate this claim, three key principles must be examined: transparency, accountability, and fairness.\n",
        "\n",
        "Firstly, transparency is limited in this project, as it is a commercial endeavor conducted for a political party. As such, the models developed will likely not be fully public. \n",
        "\n",
        "Moreover, the project focuses on the sensitive topic of COVID-related conspiracy theories, which may upset some members of the public. Explainability of the models may also be challenging, as state-of-the-art natural language processing predominantly employs neural networks, whose inner workings are relatively opaque and difficult to interpret. \n",
        "\n",
        "Additionally, the diverse data sources may make it challenging to publish input data transparently because of the different legal requirements imposed by the providers of the data.\n",
        "\n",
        "Secondly, accountability is another concern. The determination of what constitutes a conspiracy theory may be subjective, making it more difficult for external parties to validate the project's outcomes. \n",
        "\n",
        "The task of analyzing and categorizing COVID-related content on social media is inherently complex due to the intricate nature of language processing, which poses challenges for the assessment of accountability. Furthermore, the reproducibility of the procedure might be difficult to achieve given the proprietary nature of the input data and the substantial computational overhead typically associated with developing complex natural language processing models.\n",
        "\n",
        "The robustness of the models, referring to their consistency and accuracy, could be negatively impacted by the diverse nature of the input data. User-generated content on platforms like Reddit and Instagram tends to vary in format, with subtle differences in language use. \n",
        "\n",
        "In terms of the choice of output, a simple binary classification might not adequately capture the nuances in sentiment associated with conspiracy theories, thus limiting the algorithm's effectiveness. However, a more detailed sentiment analysis could be too complex and impenetrable for the end-users, including the political party client and potential external reviewers. \n",
        "\n",
        "Thirdly, fairness is a crucial issue in this project. The potential for bias is significant, as the political affiliation of the client may influence the results. For instance, they could display confirmation bias, where data that confirms their existing beliefs about what constitutes a \"conspiracy\" is favoured. Moreover, since the client is a political party, there might be a tendency, intentional or not, to flag content as a conspiracy if it goes against the party's ideology or to overlook potential conspiracies that align with the party's views. \n",
        "\n",
        "However, the ethnically diverse team is a positive factor in mitigating cultural biases.\n",
        "\n",
        "Another area of concern is that utilising data from sources like Facebook, which does not allow pseudonymity, may be problematic, as it can reveal personal information. \n",
        "\n",
        "To enhance the fairness aspect of the project, the team should implement a comprehensive bias assessment and mitigation pipeline. This would include the following steps:\n",
        "\n",
        "1. Identify potential sources of bias in the data, such as underrepresentation of certain demographic groups or communities.\n",
        "\n",
        "2. Develop and apply techniques to mitigate these biases, such as re-sampling, re-weighting, or adjusting the model's training process. Possible discrimnatory proxy variables need to be established and eliminated.\n",
        "3.Continuously monitor and evaluate the model's performance concerning fairness and adjust the mitigation strategies as needed. \n",
        "\n",
        "In conclusion, addressing the quality and limitations of the data is crucial to ensure the ethical integrity of this project. By focusing on improving transparency, accountability, and fairness, the project team can create more reliable and trustworthy outcomes while mitigating potential biases.\n",
        "\n",
        "\n",
        "***References:***\n",
        "\n",
        "Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). 'Machine bias: There's software used across the country to predict future criminals. And it's biased against blacks.' ProPublica, 23. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "Diakopoulos, N. (2015). 'Algorithmic accountability: Journalistic investigation of computational power structures.' Digital Journalism, 3(3), 398-415. https://doi.org/10.1080/21670811.2014.976411\n",
        "\n",
        "Doshi-Velez, F., and Kim, B. (2017). 'Towards a rigorous science of interpretable machine learning.' arXiv preprint arXiv:1702.08608. https://arxiv.org/abs/1702.08608\n",
        "\n",
        "Friedler, S. A., Scheidegger, C., and Venkatasubramanian, S. (2016). 'On the (im)possibility of fairness.' arXiv preprint arXiv:1609.07236. https://arxiv.org/abs/1609.07236\n",
        "\n",
        "Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., and Kagal, L. (2018). 'Explaining explanations: An overview of interpretability of machine learning.' 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA), 80-89. https://doi.org/10.1109/DSAA.2018.00018\n",
        "\n",
        "Hardt, M., Price, E., and Srebro, N. (2016). 'Equality of opportunity in supervised learning.' Advances in Neural Information Processing Systems, 29, 3315-3323. http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning\n",
        "\n",
        "Lepri, B., Oliver, N., Letouzé, E., Pentland, A., and Vinck, P. (2018). 'Fair, transparent, and accountable algorithmic decision-making processes.' Philosophy & Technology, 31(4), 611-627. https://doi.org/10.1007/s13347-017-0279-x\n",
        "\n",
        "O'Neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books.\n",
        "\n",
        "Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). 'Why should I trust you? Explaining the predictions of any classifier.' Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135-1144. https://doi.org/10.1145/2939672.2939778"
      ]
    }
  ]
}
